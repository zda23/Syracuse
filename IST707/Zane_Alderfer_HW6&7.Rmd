---
title: "Zane_Alderfer_HW6&7"
author: "Zane"
date: "2023-11-19"
output: html_document
---

```{r setup, include=FALSE}
library(e1071)
#install.packages("naivebayes")
library(naivebayes)
library(dplyr)
library(caret)
library(ggplot2)
library(rpart)
library(rpart.plot)

train_data = read.csv("/Users/zanealderfer/Downloads/Kaggle-digit-train.csv", header = TRUE, stringsAsFactors = TRUE)
test_data = read.csv("/Users/zanealderfer/Downloads/Kaggle-digit-test-1.csv", header = TRUE, stringsAsFactors = TRUE)
train_data$label<-as.factor(train_data$label)
dim(train_data)
head(train_data)
summary(train_data)
```
```{r}
library(FactoMineR)
pca_digits = PCA(t(select(train_data,-label)))
train_data =  data.frame(train_data$label,pca_digits$var$coord)   
```
```{r}
percent <- .28
set.seed(315)
DigitSplit <- sample(nrow(train_data),nrow(train_data)*percent)
DigitDF <- train_data[DigitSplit,]
dim(DigitDF)
(head(DigitDF))
(str(DigitDF))
(nrow(DigitDF))
```

```{r}
test_data$label<-as.factor(test_data$label)
# Wont use test data, instead crossvalidation on train.
dim(test_data)
(head(test_data))
```
```{r}
# Create k-folds for k-fold cross validation 
## Number of observations
N <- nrow(DigitDF)
## Number of desired splits
kfolds <- 8
## Generate indices of holdout observations
holdout <- split(sample(1:N), 1:kfolds)
head(holdout)
```

```{r}
#Run training and Testing for each of the k-folds
AllResults<-list()
AllLabels<-list()
for (k in 1:kfolds){

DF_test_data <- DigitDF[holdout[[k]], ]
DF_train_data <- DigitDF[-holdout[[k]], ]
#view(DF_train_data)
## View the created Test and Train sets
#(head(DigitDF_Train))
#(table(DigitDF_Test$DigitTotalDF.label))
## Make sure you take the labels out of the testing data
#
test_data_nolabel<-DF_test_data[-c(1)]
test_data_justlabel<-DF_test_data$train_data.label

#(head(DigitDF_Test_noLabel))

#### Naive Bayes prediction ussing e1071 package
#Naive Bayes Train model
train_naibayes<-naiveBayes(train_data.label~., data=DF_train_data, na.action = na.pass)
#train_naibayes
#summary(train_naibayes)

#Naive Bayes model Prediction 
nb_Pred <- predict(train_naibayes, test_data_nolabel)
#nb_Pred


#Testing accurancy of naive bayes model with Kaggle train data sub set
(confusionMatrix(nb_Pred, DF_test_data$train_data.label))

# Accumulate results from each fold, if you like
AllResults<- c(AllResults, nb_Pred)
AllLabels<- c(AllLabels, test_data_justlabel)
  

##Visualize
plot(nb_Pred, ylab = "Density", main = "Naive Bayes Plot")

}
```
```{r}
#Creating an index for the tree model
indexes = sample(1:50, .65*length(1:50))
indexes = c(indexes, sample(51:nrow(DF_train_data), .65*length(51:nrow(DF_train_data))))
#view(test_data)
tree_model1 = rpart(train_data.label  ~ . , data = DF_train_data[indexes,], 
                    method = 'class',
                    control = rpart.control(minbucket = 1, minsplit = 1, cp = -1),
                    model = T)
rsq.rpart(tree_model1)
rpart.plot(tree_model1)
```

```{r}
### end crossvalidation -- present results for all folds   
(table(unlist(AllResults),unlist(AllLabels)))

#########Testing with Kaggle sample######### 
test_data_nolabel<-DF_test_data[-c(1)]
(head(test_data_nolabel))

nb_Pred <- predict(train_naibayes, test_data_nolabel)
nb_Pred

### Export naive Bayesbest result and run through Kaggle

nbTestPred <- predict(train_naibayes,DF_test_data, type = 'class')
nbTestPred <- data.frame(nbTestPred)
colnames(nbTestPred)[1] <- 'Label'
nbTestPred$ImageId <- 1:nrow(nbTestPred)
nbTestPred <- nbTestPred %>% select(ImageId, Label)

#write.csv(nbTestPred, 'Naive Bayes Classifier.csv', row.names = FALSE)
```

```{r}
knitr::opts_chunk$set(echo = TRUE)
```
```{r}
train_data_2 = read.csv("/Users/zanealderfer/Downloads/Kaggle-digit-train.csv")
test_data_2 = read.csv("/Users/zanealderfer/Downloads/Kaggle-digit-test-1.csv")
train_data_2$label = factor(train_data_2$label)
#install.packages("sqldf")
library(sqldf)
library(ggplot2)
library(class)
library(e1071)
library(randomForest)
```
```{r}
#Create a random sample of n% of train data set
percent_2 <- .20
dimReduce <- .10
set.seed(300)
DigitSplit_2 <- sample(nrow(train_data_2),nrow(train_data_2)*percent_2)

train_data_2 <- train_data_2[DigitSplit_2,]
dim(train_data_2)



# Setting static variables used throughout the Models section
N <- nrow(train_data_2)
kfolds_2 <- 2
set.seed(40)
holdout <- split(sample(1:N), 1:kfolds_2)

# Function for model evaluation
get_accuracy_rate <- function(results_table, total_cases) {
    diagonal_sum <- sum(c(results_table[[1]], results_table[[12]], results_table[[23]], results_table[[34]],
                        results_table[[45]], results_table[[56]], results_table[[67]], results_table[[78]],
                        results_table[[89]], results_table[[100]]))
  (diagonal_sum / total_cases)*100
}
```

```{r}
# Discretizing
binarized_trainset <- train_data_2
for (col in colnames(binarized_trainset)) {
  if (col != "label") {
    binarized_trainset[, c(col)] <- ifelse(binarized_trainset[, c(col)] > 131, 1, 0)
  }
}
for (col in colnames(binarized_trainset)) {
  if (col != "label") {
    binarized_trainset[, c(col)] <- as.factor(binarized_trainset[, c(col)])
  }
}
```

```{r}
digit_freq <- sqldf("SELECT label, COUNT(label) as count
                     FROM train_data_2
                     GROUP BY label")
ggplot(digit_freq, aes(x=reorder(label, -count), y=count)) + geom_bar(stat="identity") + xlab("Written Digit") + ylab("Frequency Count") + ggtitle("Written Digit by Frequency Count")


zero <- 0
fifty <- 0
one_hundred <- 0
one_hundred_fifty <- 0
two_hundred <- 0
two_hundred_fifty_five <- 0
for (col in colnames(train_data_2)) {
  if (col != "label") {
    #binarized_trainset[,c(col)] <- ifelse(binarized_trainset[,c(col)] > 131, 1, 0)
    ifelse(train_data_2[,c(col)] == 0, zero <- zero + 1, ifelse(
      train_data_2[,c(col)] < 51, fifty <- fifty + 1, ifelse(
        train_data_2[,c(col)] < 101, one_hundred <- one_hundred + 1, ifelse(
          train_data_2[,c(col)] < 151, one_hundred_fifty <- one_hundred_fifty + 1, ifelse(
            train_data_2[,c(col)] < 201, two_hundred <- two_hundred + 1, two_hundred_fifty_five + 1
          )
        )
      )
    )
  )
  }
}

color_bins <- data.frame("color_bin"=c("0", "50", "100", "150", "200", "255"),
                         "count"=c(zero, fifty, one_hundred, one_hundred_fifty, two_hundred, two_hundred_fifty_five))
ggplot(color_bins, aes(x=reorder(color_bin, -count), y=count)) + geom_bar(stat="identity") + xlab("Color Bin") + ylab("Frequency Count") + ggtitle("Color Bin by Frequency Count")
```
```{r}
color_freq <- data.frame("0"=c(), "1"=c())
for (col in colnames(binarized_trainset)) {
  if (col != "label") {
    zero <- c(length(which(binarized_trainset[,c(col)] == 0)))
    one <- c(length(which(binarized_trainset[,c(col)] == 1)))
    color_freq <- rbind(color_freq, data.frame("0"=zero, "1"=one))
  }
}
colnames(color_freq) <- c("zero", "one")
color_freq <- data.frame("number"=c("zero", "one"), "count"=c(sum(color_freq$zero), sum(color_freq$one)))

ggplot(color_freq, aes(x=number, y=count)) + geom_bar(stat="identity") + xlab("Color Number") + ylab("Count") + ggtitle("Color Number by Count")
```
```{r}
k_guess = 7
all_results <- data.frame(orig=c(), pred=c())
for (k in 1:kfolds_2) {
  new_test <- train_data_2[holdout[[k]], ]
  new_train <- train_data_2[-holdout[[k]], ]
  
  new_test_no_label <- new_test[-c(1)]
  new_test_just_label <- new_test[c(1)]
  
  pred <- knn(train=new_train, test=new_test, cl=new_train$label, k=k_guess, prob=FALSE)
  
  all_results <- rbind(all_results, data.frame(orig=new_test_just_label$label, pred=pred))
}
table(all_results$orig, all_results$pred)
get_accuracy_rate(table(all_results$orig, all_results$pred), length(all_results$pred))

k_guess = 3# round(sqrt(nrow(trainset)))
all_results <- data.frame(orig=c(), pred=c())
for (k in 1:kfolds_2) {
  new_test <- train_data_2[holdout[[k]], ]
  new_train <- train_data_2[-holdout[[k]], ]
  
  new_test_no_label <- new_test[-c(1)]
  new_test_just_label <- new_test[c(1)]
  
  pred <- knn(train=new_train, test=new_test, cl=new_train$label, k=k_guess, prob=FALSE)
  
  all_results <- rbind(all_results, data.frame(orig=new_test_just_label$label, pred=pred))
}
table(all_results$orig, all_results$pred)
get_accuracy_rate(table(all_results$orig, all_results$pred), length(all_results$pred))

k_guess = 5# round(sqrt(nrow(trainset)))
all_results <- data.frame(orig=c(), pred=c())
for (k in 1:kfolds_2) {
  new_test <- train_data_2[holdout[[k]], ]
  new_train <- train_data_2[-holdout[[k]], ]
  
  new_test_no_label <- new_test[-c(1)]
  new_test_just_label <- new_test[c(1)]
  
  pred <- knn(train=new_train, test=new_test, cl=new_train$label, k=k_guess, prob=FALSE)
  
  all_results <- rbind(all_results, data.frame(orig=new_test_just_label$label, pred=pred))
}
table(all_results$orig, all_results$pred)
get_accuracy_rate(table(all_results$orig, all_results$pred), length(all_results$pred))

k_guess = 8# round(sqrt(nrow(trainset)))
all_results <- data.frame(orig=c(), pred=c())
for (k in 1:kfolds_2) {
  new_test <- train_data_2[holdout[[k]], ]
  new_train <- train_data_2[-holdout[[k]], ]
  
  new_test_no_label <- new_test[-c(1)]
  new_test_just_label <- new_test[c(1)]
  
  pred <- knn(train=new_train, test=new_test, cl=new_train$label, k=k_guess, prob=FALSE)
  
  all_results <- rbind(all_results, data.frame(orig=new_test_just_label$label, pred=pred))
}
table(all_results$orig, all_results$pred)
get_accuracy_rate(table(all_results$orig, all_results$pred), length(all_results$pred))
```
```{r}
cols_to_remove = c()
for (col in colnames(train_data_2)) {
  if (col != "label") {
    if (length(unique(train_data_2[, c(col)])) == 1) {
      cols_to_remove <- c(cols_to_remove, col)
    }
  }
}

svm_trainset <- train_data_2[-which(colnames(train_data_2) %in% cols_to_remove)]
```
```{r}
# Baseline SVM - no changes to data
all_results <- data.frame(orig=c(), pred=c())
for (k in 1:kfolds_2) {
  new_test <- svm_trainset[holdout[[k]], ]
  new_train <- svm_trainset[-holdout[[k]], ]
  
  new_test_no_label <- new_test[-c(1)]
  new_test_just_label <- new_test[c(1)]
  
  test_model <- svm(label ~ ., new_train, na.action=na.pass)
  pred <- predict(test_model, new_test_no_label, type=c("class"))
  
  all_results <- rbind(all_results, data.frame(orig=new_test_just_label$label, pred=pred))
}
table(all_results$orig, all_results$pred)
get_accuracy_rate(table(all_results$orig, all_results$pred), length(all_results$pred))
```
```{r}
binarized_svm_trainset <- svm_trainset
for (col in colnames(binarized_svm_trainset)) {
  if (col != "label") {
    binarized_svm_trainset[, c(col)] <- ifelse(binarized_svm_trainset[, c(col)] > 131, 1, 0)
  }
}
for (col in colnames(binarized_svm_trainset)) {
  if (col != "label") {
    binarized_svm_trainset[, c(col)] <- as.factor(binarized_svm_trainset[, c(col)])
  }
}

cols_to_remove = c()
for (col in colnames(binarized_svm_trainset)) {
  if (col != "label") {
    if (length(unique(binarized_svm_trainset[, c(col)])) == 1) {
      cols_to_remove <- c(cols_to_remove, col)
    }
  }
}

binarized_svm_trainset <- binarized_svm_trainset[-which(colnames(binarized_svm_trainset) %in% cols_to_remove)]
```

```{r}
all_results <- data.frame(orig=c(), pred=c())
for (k in 1:kfolds_2) {
  new_test <- binarized_svm_trainset[holdout[[k]], ]
  new_train <- binarized_svm_trainset[-holdout[[k]], ]
  
  new_test_no_label <- new_test[-c(1)]
  new_test_just_label <- new_test[c(1)]
  
  test_model <- svm(label ~ ., new_train, na.action=na.pass)
  pred <- predict(test_model, new_test_no_label, type=c("class"))
  
  all_results <- rbind(all_results, data.frame(orig=new_test_just_label$label, pred=pred))
}
table(all_results$orig, all_results$pred)
get_accuracy_rate(table(all_results$orig, all_results$pred), length(all_results$pred))
```
```{r}
all_results <- data.frame(orig=c(), pred=c())
for (k in 1:kfolds_2) {
  new_test <- binarized_svm_trainset[holdout[[k]], ]
  new_train <- binarized_svm_trainset[-holdout[[k]], ]
  
  new_test_no_label <- new_test[-c(1)]
  new_test_just_label <- new_test[c(1)]
  
  test_model <- svm(label ~ ., new_train, kernel="polynomial", na.action=na.pass)
  pred <- predict(test_model, new_test_no_label, type=c("class"))
  
  all_results <- rbind(all_results, data.frame(orig=new_test_just_label$label, pred=pred))
}
table(all_results$orig, all_results$pred)
get_accuracy_rate(table(all_results$orig, all_results$pred), length(all_results$pred))
```
```{r}
all_results <- data.frame(orig=c(), pred=c())
for (k in 1:kfolds_2) {
  new_test <- train_data_2[holdout[[k]], ]
  new_train <- train_data_2[-holdout[[k]], ]
  
  new_test_no_label <- new_test[-c(1)]
  new_test_just_label <- new_test[c(1)]
  
  test_model <- randomForest(label ~ ., new_train, na.action=na.pass)
  pred <- predict(test_model, new_test_no_label, type=c("class"))
  
  all_results <- rbind(all_results, data.frame(orig=new_test_just_label$label, pred=pred))
}
table(all_results$orig, all_results$pred)
get_accuracy_rate(table(all_results$orig, all_results$pred), length(all_results$pred))
```
```{r}
prev_result <- 0
best_result <- 0
best_number_trees <-0
for (trees in 5:15) {
  if (trees %% 5 == 0) {
    all_results <- data.frame(orig=c(), pred=c())
    for (k in 1:kfolds_2) {
      new_test <- train_data_2[holdout[[k]], ]
      new_train <- train_data_2[-holdout[[k]], ]
      
      new_test_no_label <- new_test[-c(1)]
      new_test_just_label <- new_test[c(1)]
      
      test_model <- randomForest(label ~ ., new_train, replace=TRUE, na.action=na.pass)
      pred <- predict(test_model, new_test_no_label, type=c("class"))
      
      all_results <- rbind(all_results, data.frame(orig=new_test_just_label$label, pred=pred))
    }
    #table(all_results$orig, all_results$pred)
    new_result <- get_accuracy_rate(table(all_results$orig, all_results$pred), length(all_results$pred))
    
    if (new_result > prev_result) {
      prev_result <- new_result
    } else {
      best_number_trees <- trees
      best_result <- new_result
      break
    }
  }
}  
paste("Best Number of Trees:", best_number_trees, "- Best Result:", best_result, sep=" ")
table(all_results$orig, all_results$pred)
```

