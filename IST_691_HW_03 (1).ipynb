{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZ7cPfCty5U3"
   },
   "source": [
    "# IST 691: Deep Learning in Practice\n",
    "\n",
    "**Homework 3**\n",
    "\n",
    "Name:\n",
    "\n",
    "SUID:\n",
    "\n",
    "*Save this notebook into your Google Drive. The notebook has appropriate comments at the top of code cells to indicate whether you need to modify them or not. Answer your questions directly in the notebook. Remember to use the GPU as your runtime. Once finished, run ensure all code blocks are run, download the notebook and submit through Blackboard.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8swbTwkCunA"
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "60F0x-u40An9"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# to build nearest neighbor model\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3AEhdYAMBPN"
   },
   "source": [
    "In this homework, we will perform **sarcasm detection** with [Onion](https://www.theonion.com/) vs [HuffPost](https://www.huffpost.com/) headlines, using LSTM. We will first load the data and generate the training and testing input and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JlmQdvJtL_Si"
   },
   "outputs": [],
   "source": [
    "! wget -nc -q https://github.com/mrech/NLP_TensorFlow/blob/master/0_Sentiment_in_Text/Sarcasm_Headlines_Dataset_v2.json?raw=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fzE7kwA0MdYc"
   },
   "outputs": [],
   "source": [
    "# read the downloaded dataset\n",
    "df = pd.read_json('Sarcasm_Headlines_Dataset_v2.json?raw=true', lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oefWYDEBBoQz"
   },
   "outputs": [],
   "source": [
    "# get information about the data frame\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4zt5i54PMfLF"
   },
   "outputs": [],
   "source": [
    "# take a peek at the key data\n",
    "df[['headline', 'is_sarcastic']].head(5).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TxA-gljoMt4G"
   },
   "outputs": [],
   "source": [
    "# the training input sequence will be in variable seq_padd_train and the label in train_y\n",
    "# The testing input sequence will be in variable seq_padd_test and the label in test_y\n",
    "headlines = df['headline'].values.tolist()\n",
    "sarcastic = df['is_sarcastic'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8L2cvpK9Mt4H"
   },
   "outputs": [],
   "source": [
    "training_size = 20000\n",
    "test_size = 6709\n",
    "\n",
    "train_x = headlines[:training_size]\n",
    "test_x = headlines[training_size:]\n",
    "train_y = np.array(sarcastic[:training_size])\n",
    "test_y = np.array(sarcastic[training_size:])\n",
    "\n",
    "# sequence of words input\n",
    "max_len = 16\n",
    "\n",
    "tokenizer = Tokenizer(oov_token = '<OOV>')\n",
    "tokenizer.fit_on_texts(train_x)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "index_word = {v: k for k, v in word_index.items()}\n",
    "vocab_size = len(word_index)\n",
    "sequence_train = tokenizer.texts_to_sequences(train_x)\n",
    "seq_padd_train = pad_sequences(sequence_train,\n",
    "                               padding = 'post',\n",
    "                               truncating = 'post',\n",
    "                               maxlen = max_len)\n",
    "\n",
    "\n",
    "sequence_test = tokenizer.texts_to_sequences(test_x)\n",
    "seq_padd_test = pad_sequences(sequence_test, padding = 'post',\n",
    "                              truncating = 'post',\n",
    "                              maxlen = max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9GPBdovEzBmE"
   },
   "source": [
    "### Q1 Calculating the Trainable Parameters of an LSTM\n",
    "\n",
    "Below is the summary of an LSTM neural network with embeddings and three layers. Explain in detail, after this cell, the \"why\" of the number of parameters of each of the layers displayed by `model1.summary()`. Cite any sources you used to answer this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ch0XFkqgvgg"
   },
   "source": [
    "`model1.summary()`\n",
    "```\n",
    "Model: \"model\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "input_1 (InputLayer)         [(None, None)]            0         \n",
    "_________________________________________________________________\n",
    "embedding (Embedding)        (None, None, 100)         2000100   \n",
    "_________________________________________________________________\n",
    "lstm (LSTM)                  (None, None, 128)         117248    \n",
    "_________________________________________________________________\n",
    "lstm_1 (LSTM)                (None, None, 96)          86400     \n",
    "_________________________________________________________________\n",
    "lstm_2 (LSTM)                (None, 64)                41216     \n",
    "_________________________________________________________________\n",
    "predictions (Dense)          (None, 1)                 65        \n",
    "=================================================================\n",
    "Total params: 2,245,029\n",
    "Trainable params: 2,245,029\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejPWV0vj0J34"
   },
   "source": [
    "**Why do we have the number of parameters after each of the layers?**\n",
    "\n",
    "*answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3JcopEi-R6f"
   },
   "source": [
    "### Q2: LSTM for Detecting Sarcasm\n",
    "\n",
    "Modify the code below to create an embedding layer of dimension 50. The vocabulary size is in variable `vocab_size`, and remember to add one in the embedding for the \"out of vocabulary\" input. Define an LSTM with two layers, one with 64 memory size and the second with 32 memory size. Remember to use the suffix `2` for each of the variables you define (e.g., `x2`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eJ72k-16EJA9"
   },
   "outputs": [],
   "source": [
    "# an integer input for vocab indices\n",
    "inputs2 = tf.keras.Input(shape = (None,), dtype = 'int32')\n",
    "\n",
    "# define the layers below Embedding -> LSTM 1 -> LSTM 2\n",
    "x2 = ?\n",
    "\n",
    "x2 = ?\n",
    "x2 = ?\n",
    "\n",
    "# we project onto a single unit output layer, and squash it with a sigmoid\n",
    "predictions2 = layers.Dense(1, activation = 'sigmoid', name = 'predictions')(x2)\n",
    "\n",
    "model2 = tf.keras.Model(inputs2, predictions2, name = 'lstm_simple')\n",
    "\n",
    "# compile the model with binary crossentropy loss and an adam optimizer\n",
    "model2.compile(loss = 'binary_crossentropy',\n",
    "               optimizer = 'adam',\n",
    "               metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HtQCIAoOEREC"
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "# fit the model using the train and test datasets\n",
    "model2.fit(seq_padd_train, train_y,\n",
    "           validation_split = 0.1,\n",
    "           epochs = epochs,\n",
    "           verbose = 2,\n",
    "           batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pSQR8VMIEYur"
   },
   "outputs": [],
   "source": [
    "# estimate the test performance\n",
    "model2.evaluate(seq_padd_test, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOvzWswaN3_g"
   },
   "source": [
    "### Q3: GloVe Word Embeddings\n",
    "\n",
    "Use the code below to download the GloVe embeddings and create the matrix `embedding_matrix` corresponding to the vocabulary above. Define a layer `embedding_layer_glove` which will be use by the LSTM below. Evaluate the performance and compare to model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XNWYqooOL2Km"
   },
   "outputs": [],
   "source": [
    "! wget http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WzplxdY7Oi5I"
   },
   "outputs": [],
   "source": [
    "! unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EgIsgRm-Ok1h"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "embeddings_index = {}\n",
    "f = open('glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype = 'float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mkd0RxToOtcB"
   },
   "outputs": [],
   "source": [
    "num_tokens = vocab_size + 2\n",
    "embedding_dim3 = 100\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim3))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        # this includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VGynV5Vla4s0"
   },
   "source": [
    "Create the embedding layer below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qEG5zcMhPatR"
   },
   "outputs": [],
   "source": [
    "# create the embedding layer using the embedding_matrix from above\n",
    "embedding_layer_glove = layers.Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim3,\n",
    "    input_length = max_len,\n",
    "    embeddings_initializer = tf.keras.initializers.Constant(embedding_matrix),\n",
    "    trainable = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6UHnlij6Plr4"
   },
   "outputs": [],
   "source": [
    "# an integer input for vocab indices\n",
    "inputs3 = tf.keras.Input(shape = (None,), dtype = 'int32')\n",
    "\n",
    "# next, we add a layer to map those vocab indices into a space of dimensionality\n",
    "x3 = embedding_layer_glove(?)\n",
    "\n",
    "x3 = layers.LSTM(32)(x3)\n",
    "\n",
    "# we project onto a single unit output layer, and squash it with a sigmoid\n",
    "predictions3 = layers.Dense(1, activation = 'sigmoid', name = 'predictions')(x3)\n",
    "\n",
    "model3 = tf.keras.Model(inputs3, predictions3)\n",
    "\n",
    "# compile the model with binary crossentropy loss and an adam optimizer.\n",
    "model3.compile(loss = 'binary_crossentropy',\n",
    "               optimizer = 'adam',\n",
    "               metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "izs4Y_kgPsPG"
   },
   "outputs": [],
   "source": [
    "# fit the model using the train and test datasets\n",
    "epochs = 10\n",
    "model3.fit(seq_padd_train, train_y,\n",
    "           validation_split = 0.1,\n",
    "           epochs = epochs,\n",
    "           verbose = 2,\n",
    "           batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TxwFJaqyPuxm"
   },
   "outputs": [],
   "source": [
    "model3.evaluate(seq_padd_test, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pj7HmAtdbbzh"
   },
   "source": [
    "Is it better or worse performance compared to `model2`? Why?\n",
    "\n",
    "*answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6y6P_ep9P5r3"
   },
   "source": [
    "### Q4: Word Analogies\n",
    "\n",
    "Above, we created the matrix `embedding_matrix` for the vocabulary in the sarcasm dataset. Use the code below to find the word analogy to \"`germany` is to `berlin` as `uk` is to _blank_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_u99fXdhPzRF"
   },
   "outputs": [],
   "source": [
    "# we will first create the nearest neighbor model\n",
    "nbrs_glove = NearestNeighbors(n_neighbors = 5, metric = 'cosine').fit(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0YSx1cpsQa2P"
   },
   "outputs": [],
   "source": [
    "# let's check if it works\n",
    "embedding_man = embedding_matrix[word_index['man']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vlamFoD3QThr"
   },
   "outputs": [],
   "source": [
    "# closest words to `man`\n",
    "dist, idx = nbrs_glove.kneighbors([embedding_man])\n",
    "[index_word[i] for i in idx[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xWGeWlZjQpd9"
   },
   "outputs": [],
   "source": [
    "# now define the proper embedding to solve the analogy\n",
    "blank_embedding = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aCoYOiNPRif1"
   },
   "outputs": [],
   "source": [
    "# find the closest to blank_embedding\n",
    "# closest words to `man`\n",
    "dist, idx = nbrs_glove.kneighbors([blank_embedding])\n",
    "[index_word[i] for i in idx[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6Py0Xc3SFln"
   },
   "source": [
    "### Q5: Biases\n",
    "\n",
    "As we discussed in class, there might be several biases in word embeddings. Use the list of occupations below and for each of them find whether `man` or `woman` is closest to it. In particular, first list all occupations that are closer to `man` than `woman`, and then all occupations that are closer to `woman` than `man`.\n",
    "\n",
    "_Hint_: Use the `cosine` distance between pairs of embeddings from the `SciPy` package. If the ocupation does not exist in the embedding matrix, skip it. Also, remember that the cosine distance is smaller when the embeddings are more similar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gr4lsaweTPG1"
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "print('cosine([1,1], [1,1]): ', cosine([1,1], [1,1]))\n",
    "print('cosine([1,1], [0,1]): ', cosine([1,1], [0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J1wk0iujRmyd"
   },
   "outputs": [],
   "source": [
    "occupation_list = \"\"\"technician, accountant, supervisor, engineer, worker, educator, clerk, counselor,\n",
    "inspector, mechanic, manager, therapist, administrator, salesperson, receptionist, librarian,\n",
    "advisor, pharmacist, janitor, psychologist, physician, carpenter, nurse, investigator,\n",
    "bartender, specialist, electrician, officer, pathologist, teacher, lawyer, planner, practitioner,\n",
    "plumber, instructor, surgeon, veterinarian, paramedic, examiner, chemist, machinist,\n",
    "appraiser, nutritionist, architect, hairdresser, baker, programmer, paralegal, hygienist,\n",
    "scientist\"\"\".replace('\\n', '').replace(' ', '').split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LRIFdlWFSw_q"
   },
   "outputs": [],
   "source": [
    "man_embedding = embedding_matrix[word_index['man']]\n",
    "woman_embedding = embedding_matrix[word_index['woman']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OEMLYEn4S_l3"
   },
   "outputs": [],
   "source": [
    "# first print the ocupations that are for a man, as perceived by GloVe\n",
    "for occupation in occupation_list:\n",
    "  ???\n",
    "print(???)\n",
    "# second print the ocupations that are for a woman, as perceived by GloVe\n",
    "for occupation in occupation_list:\n",
    "  ???\n",
    "print(???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NK72utxcUnVd"
   },
   "source": [
    "Do you see a pattern in the results? Do you think there are biases?\n",
    "\n",
    "*answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nr26TxJncSI8"
   },
   "source": [
    "### Q6: Sequence to Sequence Embedding\n",
    "\n",
    "What is the problem with LSTM models, and why do we need **attention** to fix them? Give as an example of what happens with sequence to sequence models for translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8VFfCmAcdmh"
   },
   "source": [
    "*answer here*"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "9GPBdovEzBmE",
    "r3JcopEi-R6f",
    "lOvzWswaN3_g",
    "6y6P_ep9P5r3",
    "s6Py0Xc3SFln",
    "nr26TxJncSI8"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
