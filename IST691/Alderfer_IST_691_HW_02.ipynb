{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42bwvaz-A9AA"
   },
   "source": [
    "# IST 691 Deep Learning in Practice\n",
    "\n",
    "**Homework 2**\n",
    "\n",
    "Name:Zane Alderfer\n",
    "\n",
    "SUID:503765874\n",
    "\n",
    "*Save this notebook into your Google Drive. The notebook has appropriate comments at the top of code cells to indicate whether you need to modify them or not. Answer your questions directly in the notebook. Remember to use the GPU as your runtime. Once finished, run ensure all code blocks are run, download the notebook and submit through Blackboard.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IpmhSKf6UKmj"
   },
   "source": [
    "### Q1\n",
    "\n",
    "Explain the differences between convolutional neural networks and a multi-layer perceptron. Explain whether the following statement is true, and if true, when it could be true.\n",
    "\n",
    "'An MLP can represent the same functions as a CNN.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIpQY40pUlVy"
   },
   "source": [
    "Some of the differences between a CNN and MLP would be the structure and parameter sharing.  A CNN are specifically used for processing grid-like data such as images.  There are convolutional layers followed by pooling layers and typically end with fully connected layers.  CNNs also use parameter sharing and local connectivity and this allows the CNN to develop spatial hierarchies which allows the network to effective at identifying image.  An MLP have a more general purpose.  They have input, hidden and output layers with each layer connected to the next layer.  MLPs are used moreso for text or tabular data where the relationships between inputs and outputs are more complex.  MLPs do not use spatial arrangements as these networks deal with text not images.  Yes they technically can represent the same functions but MLPs wouldn't be the most effective choice for grid-like data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJ0fDXyhUoCx"
   },
   "source": [
    "### Q2\n",
    "\n",
    "In class, we saw an example of autoencoders being able to remove the noise of an image. Explain why this happens and what the limits of such funcionality are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "URzCo1Bjn66i"
   },
   "source": [
    "Autoencoders have encoders and decoders which train to reconstruct input data.  The encoder compresses the input image while capturing the essential parts of the image.  The decoder then takes the compressed image and attempts to reconstruct the original image from it.  Some limitations could be the amount of \"noise\" in the image or maybe dealing with certain types of noise that is too complex to deconstruct.  Autoencoders may also remove useful information during the compression process and lead to loss of essential detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rV6HB8ARWGEB"
   },
   "source": [
    "### Q3\n",
    "\n",
    "When using transfer learning models, sometimes we get better results by fine-tuning, and some other times we get better results by freezing the parameters before training. Under what circumstances should we fine-tune the model in order to get a better result? And, under what circumstances should we freeze the parameters instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHYoOL_Fn9D6"
   },
   "source": [
    "Fine-tuning is better for large datasets that are similar to trained model.  Fine-tuning is also better with task-specific features such as dealing with pre-trained models learned features with slight adjustments for the new dataset.  Fine-tuning is also great at generalizing data to avoid overfitting.  Freezing, however, is better with limited data and also better if the new dataset is significantly different from the original dataset.  Freezing is also better when computational resources are limited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMnHyNXTn_PC"
   },
   "source": [
    "### Q4: MLP vs CNN\n",
    "\n",
    "Below, there are two neural networks for classifying MNIST digits: `model_mlp`  is an MLP with no hidden layers (the smallest possible) and 7,850 parameters. Evaluate the performance of this model below.\n",
    "\n",
    "Then, define a convolutional neural network with similar a number of parameters and evaluate its performance. Can it do better? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "J9Ym1hjSn-c2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-13 16:48:06.446753: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/zanealderfer/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/Users/zanealderfer/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/Users/zanealderfer/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 0s 0us/step\n",
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                7850      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7850 (30.66 KB)\n",
      "Trainable params: 7850 (30.66 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# DO NOT MODIFY THIS CELL\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model_mlp = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape = input_shape),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(num_classes, activation = 'softmax'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_mlp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3__Yo5Y6xOvz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "422/422 [==============================] - 2s 4ms/step - loss: 0.7290 - accuracy: 0.8178 - val_loss: 0.3559 - val_accuracy: 0.9143\n",
      "Epoch 2/15\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 0.3811 - accuracy: 0.8977 - val_loss: 0.2870 - val_accuracy: 0.9253\n",
      "Epoch 3/15\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 0.3336 - accuracy: 0.9086 - val_loss: 0.2656 - val_accuracy: 0.9287\n",
      "Epoch 4/15\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 0.3110 - accuracy: 0.9140 - val_loss: 0.2512 - val_accuracy: 0.9318\n",
      "Epoch 5/15\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 0.2977 - accuracy: 0.9171 - val_loss: 0.2428 - val_accuracy: 0.9335\n",
      "Epoch 6/15\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 0.2889 - accuracy: 0.9199 - val_loss: 0.2379 - val_accuracy: 0.9362\n",
      "Epoch 7/15\n",
      "422/422 [==============================] - 1s 4ms/step - loss: 0.2825 - accuracy: 0.9210 - val_loss: 0.2332 - val_accuracy: 0.9368\n",
      "Epoch 8/15\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 0.2773 - accuracy: 0.9231 - val_loss: 0.2314 - val_accuracy: 0.9383\n",
      "Epoch 9/15\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 0.2733 - accuracy: 0.9244 - val_loss: 0.2284 - val_accuracy: 0.9385\n",
      "Epoch 10/15\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 0.2698 - accuracy: 0.9247 - val_loss: 0.2270 - val_accuracy: 0.9385\n",
      "Epoch 11/15\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 0.2667 - accuracy: 0.9255 - val_loss: 0.2286 - val_accuracy: 0.9397\n",
      "Epoch 12/15\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 0.2645 - accuracy: 0.9266 - val_loss: 0.2258 - val_accuracy: 0.9388\n",
      "Epoch 13/15\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 0.2618 - accuracy: 0.9266 - val_loss: 0.2251 - val_accuracy: 0.9395\n",
      "Epoch 14/15\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 0.2602 - accuracy: 0.9274 - val_loss: 0.2240 - val_accuracy: 0.9398\n",
      "Epoch 15/15\n",
      "422/422 [==============================] - 1s 3ms/step - loss: 0.2582 - accuracy: 0.9284 - val_loss: 0.2230 - val_accuracy: 0.9403\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f98e8ac1dc0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DO NOT MODIFY CELL\n",
    "batch_size = 128\n",
    "epochs = 15\n",
    "model_mlp.compile(loss = 'categorical_crossentropy',\n",
    "                  optimizer = 'adam',\n",
    "                  metrics = ['accuracy'])\n",
    "\n",
    "model_mlp.fit(x_train,\n",
    "              y_train,\n",
    "              batch_size = batch_size,\n",
    "              epochs = epochs,\n",
    "              validation_split = 0.1,\n",
    "              verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AleryIHdxVTE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.2641875147819519\n",
      "Test accuracy: 0.9277999997138977\n"
     ]
    }
   ],
   "source": [
    "# DO NOT MODIFY CELL\n",
    "score = model_mlp.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "PU7Y2sRVxXJZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 16)        160       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 13, 13, 16)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 11, 11, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 5, 5, 32)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 800)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               102528    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 108618 (424.29 KB)\n",
      "Trainable params: 108618 (424.29 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# DEFINE YOUR OWN CNN SO THAT THE PARAMETERS ARE FEWER THAN THE MLP\n",
    "model_cnn = keras.Sequential(\n",
    "    [\n",
    "        layers.Conv2D(16, kernel_size=(3, 3), activation=\"relu\", input_shape=input_shape),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dense(num_classes, activation = 'softmax'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "L7ZbKxvid1nt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 27, 27, 32)        160       \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 26, 26, 32)        4128      \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 3, 3, 32)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 288)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                2890      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7178 (28.04 KB)\n",
      "Trainable params: 7178 (28.04 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# one possible answer that acheives ~98% test accuracy\n",
    "# and has 7,178 trainable parameters\n",
    "model_cnn = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32,2, activation = 'relu'),\n",
    "        layers.Conv2D(32,2, activation = 'relu'),\n",
    "        layers.MaxPooling2D(8),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "hg_0a0DVyYSR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "422/422 [==============================] - 35s 80ms/step - loss: 0.7736 - accuracy: 0.7643 - val_loss: 0.2362 - val_accuracy: 0.9302\n",
      "Epoch 2/15\n",
      "422/422 [==============================] - 34s 81ms/step - loss: 0.2299 - accuracy: 0.9274 - val_loss: 0.1583 - val_accuracy: 0.9517\n",
      "Epoch 3/15\n",
      "422/422 [==============================] - 34s 81ms/step - loss: 0.1786 - accuracy: 0.9426 - val_loss: 0.1308 - val_accuracy: 0.9592\n",
      "Epoch 4/15\n",
      "422/422 [==============================] - 34s 81ms/step - loss: 0.1501 - accuracy: 0.9520 - val_loss: 0.1154 - val_accuracy: 0.9643\n",
      "Epoch 5/15\n",
      "422/422 [==============================] - 34s 80ms/step - loss: 0.1317 - accuracy: 0.9584 - val_loss: 0.1009 - val_accuracy: 0.9680\n",
      "Epoch 6/15\n",
      "422/422 [==============================] - 38s 89ms/step - loss: 0.1187 - accuracy: 0.9634 - val_loss: 0.0916 - val_accuracy: 0.9688\n",
      "Epoch 7/15\n",
      "422/422 [==============================] - 35s 83ms/step - loss: 0.1077 - accuracy: 0.9666 - val_loss: 0.0868 - val_accuracy: 0.9737\n",
      "Epoch 8/15\n",
      "422/422 [==============================] - 35s 82ms/step - loss: 0.0995 - accuracy: 0.9688 - val_loss: 0.0824 - val_accuracy: 0.9743\n",
      "Epoch 9/15\n",
      "422/422 [==============================] - 35s 83ms/step - loss: 0.0948 - accuracy: 0.9706 - val_loss: 0.0753 - val_accuracy: 0.9768\n",
      "Epoch 10/15\n",
      "422/422 [==============================] - 36s 85ms/step - loss: 0.0892 - accuracy: 0.9723 - val_loss: 0.0730 - val_accuracy: 0.9780\n",
      "Epoch 11/15\n",
      "422/422 [==============================] - 35s 83ms/step - loss: 0.0847 - accuracy: 0.9736 - val_loss: 0.0716 - val_accuracy: 0.9798\n",
      "Epoch 12/15\n",
      "422/422 [==============================] - 35s 83ms/step - loss: 0.0810 - accuracy: 0.9746 - val_loss: 0.0682 - val_accuracy: 0.9778\n",
      "Epoch 13/15\n",
      "422/422 [==============================] - 36s 84ms/step - loss: 0.0783 - accuracy: 0.9753 - val_loss: 0.0650 - val_accuracy: 0.9792\n",
      "Epoch 14/15\n",
      "422/422 [==============================] - 36s 84ms/step - loss: 0.0756 - accuracy: 0.9762 - val_loss: 0.0647 - val_accuracy: 0.9790\n",
      "Epoch 15/15\n",
      "422/422 [==============================] - 36s 85ms/step - loss: 0.0732 - accuracy: 0.9768 - val_loss: 0.0662 - val_accuracy: 0.9782\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f98ea502bb0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DO NOT MODIFY CELL\n",
    "batch_size = 128\n",
    "epochs = 15\n",
    "model_cnn.compile(loss = 'categorical_crossentropy',\n",
    "                  optimizer = 'adam',\n",
    "                  metrics = ['accuracy'])\n",
    "model_cnn.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Tbq5a6wfy_Ix"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.0721391960978508\n",
      "Test accuracy: 0.9782000184059143\n"
     ]
    }
   ],
   "source": [
    "# DO NOT MODIFY CELL\n",
    "score = model_cnn.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xdCoPk6A8wXJ"
   },
   "source": [
    "**Did the CNN do better than the MLP? Why or why not?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9ENK5oR05jB"
   },
   "source": [
    "Yes the CNN did perform better than the MLP.  This is due to CNN having better parameter effciency than the MLP network.  CNNs are generally more parameter efficient when involving grid-like data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MFZBXr4B1iBR"
   },
   "source": [
    "### Q5: Transfer learning\n",
    "\n",
    "We are going to classify beans using transfer learning (read more about the dataset [here](https://www.tensorflow.org/datasets/catalog/beans). In the code below, use the `ResNet50` model available in Keras to classify the beans dataset (3 classes). **Do not fine tune `ResNet50`**. What is the performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "dFmR2-Uv5_Ct"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-13 17:21:02.191633: W external/local_tsl/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata.google.internal\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: 171.63 MiB, total: 171.63 MiB) to /Users/zanealderfer/tensorflow_datasets/beans/0.1.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f388071313f4ede84b274413e51d186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df9374ea277848cbb73cac4395598ef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...:   0%|          | 0/1034 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /Users/zanealderfer/tensorflow_datasets/beans/0.1.0.incompleteTO5QY5/beans-train.tfrecord*...:   0%|…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation examples...:   0%|          | 0/133 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /Users/zanealderfer/tensorflow_datasets/beans/0.1.0.incompleteTO5QY5/beans-validation.tfrecord*...: …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...:   0%|          | 0/128 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /Users/zanealderfer/tensorflow_datasets/beans/0.1.0.incompleteTO5QY5/beans-test.tfrecord*...:   0%| …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset beans downloaded and prepared to /Users/zanealderfer/tensorflow_datasets/beans/0.1.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# DO NOT MODIFY CELL\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# loading images and labels\n",
    "(train_ds, train_labels), (test_ds, test_labels) = tfds.load(\n",
    "    'beans',\n",
    "    split = ['train[:70%]', 'train[:30%]'], # train/test split\n",
    "    batch_size = -1,\n",
    "    as_supervised = True  # include labels\n",
    ")\n",
    "\n",
    "# resizing images\n",
    "train_ds = tf.image.resize(train_ds, (200, 200))\n",
    "test_ds = tf.image.resize(test_ds, (200, 200))\n",
    "\n",
    "# transforming labels to correct format\n",
    "train_labels = to_categorical(train_labels, num_classes=3)\n",
    "test_labels = to_categorical(test_labels, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "iHFh3uyU5X99"
   },
   "outputs": [],
   "source": [
    "# IMPORT THE APPROPRIATE MODEL HERE\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "## loading ResNet50 model\n",
    "base_model = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(200, 200, 3))\n",
    "base_model.trainable = False\n",
    "\n",
    "## preprocessing input\n",
    "train_ds = preprocess_input(train_ds)\n",
    "test_ds = preprocess_input(test_ds)\n",
    "\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "flatten_layer = layers.Flatten()\n",
    "prediction_layer = layers.Dense(3, activation = 'softmax')\n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    flatten_layer,\n",
    "    layers.Dropout(0.2),\n",
    "    prediction_layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "exmwjiyT5wa4"
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY CELL\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow import keras\n",
    "\n",
    "model.compile(\n",
    "    optimizer = keras.optimizers.Adam(),\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "fRBrMvl65zQ3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "10/10 [==============================] - 82s 8s/step - loss: 5.5374 - accuracy: 0.5976 - val_loss: 5.7108 - val_accuracy: 0.7103\n",
      "Epoch 2/5\n",
      "10/10 [==============================] - 71s 7s/step - loss: 1.4625 - accuracy: 0.8584 - val_loss: 5.1462 - val_accuracy: 0.7172\n",
      "Epoch 3/5\n",
      "10/10 [==============================] - 70s 7s/step - loss: 0.3383 - accuracy: 0.9568 - val_loss: 3.0000 - val_accuracy: 0.7862\n",
      "Epoch 4/5\n",
      "10/10 [==============================] - 73s 7s/step - loss: 0.1774 - accuracy: 0.9793 - val_loss: 2.9811 - val_accuracy: 0.7793\n",
      "Epoch 5/5\n",
      "10/10 [==============================] - 71s 7s/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 3.4850 - val_accuracy: 0.7862\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f98632e6460>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DO NOT MODIFY CELL\n",
    "model.fit(train_ds, train_labels, epochs = 5, validation_split = 0.2, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "_WFHScEp53b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.008390230126678944\n",
      "Test accuracy: 0.9967741966247559\n"
     ]
    }
   ],
   "source": [
    "# DO NOT MODIFY CELL\n",
    "score = model.evaluate(test_ds, test_labels, verbose = 0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uB9ltzce7mjE"
   },
   "source": [
    "### Q6: Autoencoder\n",
    "\n",
    "Modify the convolutional autoencoder for MNIST we saw in class so that the embedding has the following structure:\n",
    "- Conv2D: 8 filters, Kernel (3, 3)\n",
    "- MaxPooling: Size (2, 2)\n",
    "- Conv2D: 3 filters, Kernel (3, 3)\n",
    "- MaxPooling: Size (2, 2)\n",
    "- Conv2D: 1 filters, Kernel (3, 3)\n",
    "\n",
    "After making this change, you need to change the input size of the decoder function so that it can accept the output of the encoder. What is the performance of your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "I-kgpVhF7e_U"
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CELL\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras import backend as keras_backend\n",
    "keras_backend.set_image_data_format('channels_last')\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, Dense, Input, MaxPooling2D, UpSampling2D\n",
    "import np_utils\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "# data now has a different shape\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# load the MNIST data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "image_height = X_train.shape[1]\n",
    "image_width = X_train.shape[2]\n",
    "number_of_pixels = image_height * image_width\n",
    "\n",
    "# cast the sample data to the current Keras floating-point type\n",
    "X_train = keras_backend.cast_to_floatx(X_train)\n",
    "X_test = keras_backend.cast_to_floatx(X_test)\n",
    "\n",
    "# reshape to 2D grid, one line per image\n",
    "X_train = X_train.reshape(X_train.shape[0], number_of_pixels)\n",
    "X_test = X_test.reshape(X_test.shape[0], number_of_pixels)\n",
    "\n",
    "# scale data to range [0, 1]\n",
    "X_train /= 255.0\n",
    "X_test /= 255.0\n",
    "\n",
    "# reshape sample data to 4D tensor using channels_last convention\n",
    "X_train = X_train.reshape(X_train.shape[0], image_height, image_width, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], image_height, image_width, 1)\n",
    "\n",
    "# replace label data with one-hot encoded versions\n",
    "number_of_classes = 1 + max(np.append(y_train, y_test))\n",
    "y_train = to_categorical(y_train, number_of_classes)\n",
    "y_test = to_categorical(y_test, number_of_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "UVDXyiwM_Ry7"
   },
   "outputs": [],
   "source": [
    "# MODIFY THE ENCODER BELOW ACCORDING TO THE QUESTION REQUIREMENTS\n",
    "CAE_encoder_conv_1 = Conv2D(8, (3, 3), activation = 'relu', padding = 'same')\n",
    "CAE_encoder_pool_1 = MaxPooling2D((2,2), padding = 'same')\n",
    "CAE_encoder_conv_2 = Conv2D(3, (3, 3), activation = 'relu', padding = 'same')\n",
    "CAE_encoder_pool_2 = MaxPooling2D((2,2), padding = 'same')\n",
    "CAE_encoder_output = Conv2D(1, (3, 3), activation = 'relu', padding = 'same')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "Mbg8fctb8GX9"
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CELL\n",
    "CAE_encoder_input = Input(shape=(28, 28, 1))\n",
    "CAE_decoder_up_1 = UpSampling2D((2,2))\n",
    "CAE_decoder_conv_1 = Conv2D(8, (3, 3), activation = 'relu', padding = 'same')\n",
    "CAE_decoder_up_2 = UpSampling2D((2,2))\n",
    "CAE_decoder_output = Conv2D(1, (3, 3), activation = 'sigmoid', padding = 'same')\n",
    "\n",
    "CAE_encoder_step_1 = CAE_encoder_conv_1(CAE_encoder_input)\n",
    "CAE_encoder_step_2 = CAE_encoder_pool_1(CAE_encoder_step_1)\n",
    "CAE_encoder_step_3 = CAE_encoder_conv_2(CAE_encoder_step_2)\n",
    "CAE_encoder_step_4 = CAE_encoder_pool_2(CAE_encoder_step_3)\n",
    "CAE_encoder_step_5 = CAE_encoder_output(CAE_encoder_step_4)\n",
    "\n",
    "CAE_decoder_step_1 = CAE_decoder_up_1(CAE_encoder_step_5)\n",
    "CAE_decoder_step_2 = CAE_decoder_conv_1(CAE_decoder_step_1)\n",
    "CAE_decoder_step_3 = CAE_decoder_up_2(CAE_decoder_step_2)\n",
    "CAE_decoder_step_4 = CAE_decoder_output(CAE_decoder_step_3)\n",
    "\n",
    "\n",
    "Conv_AE = Model(CAE_encoder_input, CAE_decoder_step_4)\n",
    "Conv_AE.compile(optimizer = 'adam', loss = 'binary_crossentropy')\n",
    "\n",
    "\n",
    "Conv_AE_encoder_only_model = Model(CAE_encoder_input, CAE_encoder_step_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "eCmKgIpx9LV-"
   },
   "outputs": [],
   "source": [
    "# MODIFY THE INPUT FOR THE DECODER BELOW ACCORDING TO THE OUTPUT EXPECTED FROM THE ENCODER\n",
    "Conv_AE_decoder_only_input = Input(shape=(28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "i7unPXGF_cbi"
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CELL\n",
    "Conv_AE_decoder_only_step_1 = CAE_decoder_up_1(Conv_AE_decoder_only_input)\n",
    "Conv_AE_decoder_only_step_2 = CAE_decoder_conv_1(Conv_AE_decoder_only_step_1)\n",
    "Conv_AE_decoder_only_step_3 = CAE_decoder_up_2(Conv_AE_decoder_only_step_2)\n",
    "Conv_AE_decoder_only_step_4 = CAE_decoder_output(Conv_AE_decoder_only_step_3)\n",
    "\n",
    "Conv_AE_decoder_only_model = Model(Conv_AE_decoder_only_input, Conv_AE_decoder_only_step_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "Zp60H-tq_cuG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "469/469 - 69s - loss: 0.2592 - val_loss: 0.1600 - 69s/epoch - 147ms/step\n",
      "Epoch 2/50\n",
      "469/469 - 46s - loss: 0.1468 - val_loss: 0.1362 - 46s/epoch - 99ms/step\n",
      "Epoch 3/50\n",
      "469/469 - 36s - loss: 0.1328 - val_loss: 0.1280 - 36s/epoch - 77ms/step\n",
      "Epoch 4/50\n",
      "469/469 - 37s - loss: 0.1263 - val_loss: 0.1227 - 37s/epoch - 79ms/step\n",
      "Epoch 5/50\n",
      "469/469 - 37s - loss: 0.1224 - val_loss: 0.1198 - 37s/epoch - 79ms/step\n",
      "Epoch 6/50\n",
      "469/469 - 36s - loss: 0.1194 - val_loss: 0.1168 - 36s/epoch - 76ms/step\n",
      "Epoch 7/50\n",
      "469/469 - 40s - loss: 0.1168 - val_loss: 0.1143 - 40s/epoch - 85ms/step\n",
      "Epoch 8/50\n",
      "469/469 - 36s - loss: 0.1145 - val_loss: 0.1122 - 36s/epoch - 76ms/step\n",
      "Epoch 9/50\n",
      "469/469 - 37s - loss: 0.1128 - val_loss: 0.1107 - 37s/epoch - 79ms/step\n",
      "Epoch 10/50\n",
      "469/469 - 39s - loss: 0.1114 - val_loss: 0.1095 - 39s/epoch - 84ms/step\n",
      "Epoch 11/50\n",
      "469/469 - 35s - loss: 0.1103 - val_loss: 0.1087 - 35s/epoch - 75ms/step\n",
      "Epoch 12/50\n",
      "469/469 - 35s - loss: 0.1094 - val_loss: 0.1078 - 35s/epoch - 75ms/step\n",
      "Epoch 13/50\n",
      "469/469 - 38s - loss: 0.1087 - val_loss: 0.1072 - 38s/epoch - 81ms/step\n",
      "Epoch 14/50\n",
      "469/469 - 44s - loss: 0.1081 - val_loss: 0.1066 - 44s/epoch - 93ms/step\n",
      "Epoch 15/50\n",
      "469/469 - 44s - loss: 0.1076 - val_loss: 0.1063 - 44s/epoch - 93ms/step\n",
      "Epoch 16/50\n",
      "469/469 - 44s - loss: 0.1073 - val_loss: 0.1058 - 44s/epoch - 94ms/step\n",
      "Epoch 17/50\n",
      "469/469 - 47s - loss: 0.1070 - val_loss: 0.1057 - 47s/epoch - 100ms/step\n",
      "Epoch 18/50\n",
      "469/469 - 48s - loss: 0.1067 - val_loss: 0.1054 - 48s/epoch - 102ms/step\n",
      "Epoch 19/50\n",
      "469/469 - 52s - loss: 0.1065 - val_loss: 0.1053 - 52s/epoch - 111ms/step\n",
      "Epoch 20/50\n",
      "469/469 - 48s - loss: 0.1063 - val_loss: 0.1051 - 48s/epoch - 103ms/step\n",
      "Epoch 21/50\n",
      "469/469 - 46s - loss: 0.1062 - val_loss: 0.1048 - 46s/epoch - 99ms/step\n",
      "Epoch 22/50\n",
      "469/469 - 59s - loss: 0.1060 - val_loss: 0.1047 - 59s/epoch - 126ms/step\n",
      "Epoch 23/50\n",
      "469/469 - 78s - loss: 0.1059 - val_loss: 0.1046 - 78s/epoch - 165ms/step\n",
      "Epoch 24/50\n",
      "469/469 - 70s - loss: 0.1058 - val_loss: 0.1046 - 70s/epoch - 149ms/step\n",
      "Epoch 25/50\n",
      "469/469 - 98s - loss: 0.1058 - val_loss: 0.1045 - 98s/epoch - 208ms/step\n",
      "Epoch 26/50\n",
      "469/469 - 85s - loss: 0.1056 - val_loss: 0.1044 - 85s/epoch - 182ms/step\n",
      "Epoch 27/50\n",
      "469/469 - 75s - loss: 0.1056 - val_loss: 0.1045 - 75s/epoch - 159ms/step\n",
      "Epoch 28/50\n",
      "469/469 - 75s - loss: 0.1055 - val_loss: 0.1043 - 75s/epoch - 159ms/step\n",
      "Epoch 29/50\n",
      "469/469 - 68s - loss: 0.1054 - val_loss: 0.1042 - 68s/epoch - 145ms/step\n",
      "Epoch 30/50\n",
      "469/469 - 68s - loss: 0.1054 - val_loss: 0.1041 - 68s/epoch - 145ms/step\n",
      "Epoch 31/50\n",
      "469/469 - 65s - loss: 0.1053 - val_loss: 0.1040 - 65s/epoch - 139ms/step\n",
      "Epoch 32/50\n",
      "469/469 - 51s - loss: 0.1052 - val_loss: 0.1040 - 51s/epoch - 108ms/step\n",
      "Epoch 33/50\n",
      "469/469 - 42s - loss: 0.1052 - val_loss: 0.1041 - 42s/epoch - 89ms/step\n",
      "Epoch 34/50\n",
      "469/469 - 38s - loss: 0.1051 - val_loss: 0.1039 - 38s/epoch - 82ms/step\n",
      "Epoch 35/50\n",
      "469/469 - 37s - loss: 0.1051 - val_loss: 0.1040 - 37s/epoch - 78ms/step\n",
      "Epoch 36/50\n",
      "469/469 - 41s - loss: 0.1050 - val_loss: 0.1038 - 41s/epoch - 87ms/step\n",
      "Epoch 37/50\n",
      "469/469 - 37s - loss: 0.1050 - val_loss: 0.1038 - 37s/epoch - 78ms/step\n",
      "Epoch 38/50\n",
      "469/469 - 36s - loss: 0.1050 - val_loss: 0.1037 - 36s/epoch - 76ms/step\n",
      "Epoch 39/50\n",
      "469/469 - 36s - loss: 0.1049 - val_loss: 0.1037 - 36s/epoch - 77ms/step\n",
      "Epoch 40/50\n",
      "469/469 - 40s - loss: 0.1049 - val_loss: 0.1036 - 40s/epoch - 86ms/step\n",
      "Epoch 41/50\n",
      "469/469 - 38s - loss: 0.1048 - val_loss: 0.1042 - 38s/epoch - 82ms/step\n",
      "Epoch 42/50\n",
      "469/469 - 41s - loss: 0.1048 - val_loss: 0.1035 - 41s/epoch - 88ms/step\n",
      "Epoch 43/50\n",
      "469/469 - 46s - loss: 0.1047 - val_loss: 0.1036 - 46s/epoch - 98ms/step\n",
      "Epoch 44/50\n",
      "469/469 - 37s - loss: 0.1048 - val_loss: 0.1035 - 37s/epoch - 79ms/step\n",
      "Epoch 45/50\n",
      "469/469 - 37s - loss: 0.1047 - val_loss: 0.1035 - 37s/epoch - 79ms/step\n",
      "Epoch 46/50\n",
      "469/469 - 36s - loss: 0.1047 - val_loss: 0.1035 - 36s/epoch - 77ms/step\n",
      "Epoch 47/50\n",
      "469/469 - 35s - loss: 0.1047 - val_loss: 0.1034 - 35s/epoch - 75ms/step\n",
      "Epoch 48/50\n",
      "469/469 - 37s - loss: 0.1046 - val_loss: 0.1035 - 37s/epoch - 79ms/step\n",
      "Epoch 49/50\n",
      "469/469 - 37s - loss: 0.1047 - val_loss: 0.1037 - 37s/epoch - 78ms/step\n",
      "Epoch 50/50\n",
      "469/469 - 36s - loss: 0.1046 - val_loss: 0.1038 - 36s/epoch - 77ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f98f14ce250>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DO NOT MODIFY THIS CELL\n",
    "# FIT AND EVALUATE PERFORMANCE\n",
    "Conv_AE.fit(X_train, X_train,\n",
    "               epochs = 50, batch_size = 128, shuffle = True,\n",
    "               verbose = 2,\n",
    "               validation_data = (X_test, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "6XnyzumB_fnY"
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CELL\n",
    "def draw_predictions_set(predictions, filename = None):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    for i in range(5):\n",
    "        plt.subplot(2, 5, i+1)\n",
    "        plt.imshow(X_test[i].reshape(28, 28), vmin = 0, vmax = 1, cmap = 'gray')\n",
    "        ax = plt.gca()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        plt.subplot(2, 5, i + 6)\n",
    "        plt.imshow(predictions[i].reshape(28, 28), vmin = 0, vmax = 1, cmap = 'gray')\n",
    "        ax = plt.gca()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "LrxzJAIA_jHW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 4s 9ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjgAAAD2CAYAAADbCP0bAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAhBElEQVR4nO3dd7TUxRXA8XkCUg3SEUSqoEhVQhOiIKEoKEUiSiSRk4AJJJyAYAhiCAnhBHPgaASMiQSiHkiUEqSJhSpFQKq0gMIDAekdQvHlj5yM917Y52vbZr+fv+6cu2/3x/vt7htm7sykZWRkOAAAgJDcEO8LAAAAyGt0cAAAQHDo4AAAgODQwQEAAMGhgwMAAIJDBwcAAAQnf3YenJaWxpryBJGRkZGWk5/jHiYO7mEQjmZkZJTJ7g9xDxMK9zD5XfceMoIDADm3N94XgFzjHia/695DOjgAACA4dHAAAEBw6OAAAIDg0MEBAADBoYMDAACCQwcHAAAEhw4OAAAIDh0cAAAQHDo4AAAgONk6qgHIS88884xqFy5c2Mf16tVTuUcffTTi80ycOFG1V65c6ePXX389N5cIAEhSjOAAAIDg0MEBAADBScvIyPqBqJyemjiS9STqf/zjHz7ObNopN3bv3u3jNm3aqFx6enpUXjMnkvUexkLNmjV9vH37dpUbMGCAj//0pz/F7JoiWJeRkdEouz+ULPewaNGiqv3CCy/4uG/fviq3bt061e7evbuP9+5N6PMsg76HKeK695ARHAAAEBw6OAAAIDh0cAAAQHBYJo6okjU3zmW97sbWXbz77rs+rlatmsp16tRJtatXr+7jnj17qtzo0aOz9PqIr4YNG/r4q6++Urn9+/fH+nJS1i233KLaP/7xj31s78s999yj2h07dvTx+PHjo3B1+L+7775btWfMmOHjKlWqRP3127Ztq9rbtm3z8b59+6L++pEwggMAAIJDBwcAAASHKSrkuUaNvl6t16VLl4iP+/TTT1X74Ycf9vHRo0dV7uzZsz6+8cYbVW7VqlWqXb9+fR+XKlUqC1eMRNOgQQMfnzt3TuVmzpwZ46tJLWXKlPHxlClT4nglyKp27dqpdsGCBWP6+rZMoHfv3j7u0aNHTK9FYgQHAAAEhw4OAAAIDh0cAAAQnLjX4Nhlw3IZ4oEDB1Tu4sWLPn7zzTdV7tChQz7etWtXXl4iskkuLU1L06cRyLobO2988ODBLD3/oEGDVLt27doRHzt37twsPSfiq06dOqrdv39/H3MifHT9/Oc/V+3OnTv7uHHjxjl+3u985zs+vuEG/X/pjRs3+njp0qU5fo1Ulj//13++H3zwwTheybXHdAwcONDH9rgPW1MXTYzgAACA4NDBAQAAwYn7FNWYMWNUO6u7LtqTbM+cOeNju/w4FuTuqvbftHbt2lhfTly98847Pq5Ro4bKyft0/PjxHD2/XXZYoECBHD0PEscdd9yh2nJY2+6Gjbw1btw41bY7FOdU165drxs7p08Xf+yxx1TOTnfg+lq1auXjZs2aqZz9GxRtJUqUUG1ZNlCkSBGVY4oKAAAgF+jgAACA4NDBAQAAwYl7DY5cFu6cc/Xq1fOxPJHUOefuvPNOH9vTU++//34fN23aVOXkaaaVKlXK8rVduXJFtY8cOeJje8qulJ6ertqpVoMjybn23Bg8eLCPa9asmeljV69efd0YiWvIkCGqLd83qfz5iZZ58+b52C7hzqljx46ptjxepXLlyipXtWpVH3/88ccqly9fvjy5ntDYrRSmTp3q4927d6vc73//+5hc0/898sgjMX29rGIEBwAABIcODgAACE7cp6g++OCDTNvSggULIubkMjV5ErFzetnht7/97Sxfm9w52Tnndu7c6WM7fVayZEkf2+FCZF/Hjh1Ve+TIkT62p4kfPnxYtYcOHerj8+fPR+HqkFt2Owh5Ar1z+rMWy2WlobrvvvtUu1atWj62y8Kzukz8lVdeUe2FCxeq9qlTp3zcunVrlRs2bFjE5/3JT37i44kTJ2bpWlLBc889p9pyK4X27durnJwejBb5N8++v/Jqq4HcYgQHAAAEhw4OAAAIDh0cAAAQnLjX4OSVEydO+HjRokURH5dZjc836datm4/t1tSbN2/2MVvL556tybB1N5L9fS9ZsiQq14S8Y+fsLbklA3JG1jlNmzZN5UqXLp2l57DbPEyfPt3Hv/nNb1Qus3o3+zx9+vTxcZkyZVROHjNQqFAhlXv55Zd9fPny5YivF4pHH33Ux/bE8F27dvk4HlspyDoqW3OzePFiH588eTJGV3QtRnAAAEBw6OAAAIDgBDNFFQ1ly5ZV7QkTJvjY7v4plzHn9JTsVDdr1iwft23bNuLj/v73v6u2XT6JxFe3bt1M87E+DTlE+fN//fWe1Skp5/QUb48ePVTu6NGjOboWO0U1evRoH48dO1bl5OnT9n0we/ZsH6fCdhzdu3f3sT2VW/49igW7tUPPnj19fPXqVZX73e9+5+N4TiUyggMAAIJDBwcAAASHDg4AAAgONTiZ6Nevn2rL5YxyWbpzzu3YsSMm1xQSeyJ78+bNfVywYEGVk3P/cn7XudhsS47ca9q0qY+feuoplVu/fr1qv/feezG5Jly7xLh3794+zmnNzTeRtTSylsO57B2nE5rixYurtvzMWLE+xkIu7XdO13XZo4sy26ollhjBAQAAwaGDAwAAgsMUlXHvvff6+Je//GXEx3Xu3Fm1t2zZEq1LCpbcFdU550qVKhXxsW+88YaPU2F5aIjatGnjY3kSsXPOLViwQLUvXrwYk2tKFXZbC6lJkyYxvJL/SUtL87G9tsyudcSIET5+8skn8/y64s1OzVesWNHHU6dOjfXlKNWrV4+YS9S/f4zgAACA4NDBAQAAwaGDAwAAgkMNjiFPbC1QoIDKyZPIV65cGbNrCsnDDz/s47vvvjvi4+RptM459+tf/zpal4QYqV+/vo8zMjJU7u2334715QTv6aef9rE97TneOnXq5OOGDRuqnLxWe92yBidEZ86cUe0NGzb4uF69eion69iidTyQPK5InmxuLV++PCqvn1uM4AAAgODQwQEAAMGhgwMAAIKT8jU4hQsXVu327dv7+NKlSyon60DieQR8MrF72/zqV7/ysa1xkuTcs3Mcx5CMypcvr9otW7b0sT3aZObMmTG5plQi61ziQR5tU7t2bZWT3wOZOXLkiGqH/r174cIF1ZZ7fnXr1k3l5s6d6+OxY8fm6PXq1Kmj2tWqVVPtKlWq+NjWzUmJVuP1f4zgAACA4NDBAQAAwUn5KarBgwertlyyaLePX7FiRUyuKSSDBg1S7cxOCp41a5aPWRae/H74wx+qtlxyOn/+/BhfDWJt2LBhPu7Xr1+Wf27Pnj0+/sEPfqBy6enpub6uZCK/B+XxFs4599BDD/k4p8c42NPi7TSUPDE8M5MnT87R60cbIzgAACA4dHAAAEBw6OAAAIDgpFwNjpy3dM654cOHq/bp06d9PHLkyJhcU8gGDhyY5cf279/fxywLT36VK1eOmDtx4kQMrwSxMG/ePNWuVatWjp5n69atPk7UIwBiZfv27T7+3ve+p3INGjTwcY0aNXL0/N90RMqUKVN83LNnz4iPs8vbEwUjOAAAIDh0cAAAQHBSYopK7qb70ksvqVy+fPlUWw6zrlq1KroXBkWejpubHUtPnToV8Xnk7snFixeP+Bw333yzamd1qu3q1auq/eyzz/r4/PnzWXqOUHTs2DFi7p133onhlaQmuaz4hhsi/1+2Q4cOEXOvvvqqaleoUCHiY+1r5HR323jvwJws5G7vduf3vPLZZ59l6XF2R+QtW7ZE43KyjREcAAAQHDo4AAAgOHRwAABAcIKswbF1NfLIhapVq6qcPK3VuWuXjSN2Nm3alCfP89Zbb/n44MGDKleuXDkfP/bYY3nyepk5dOiQj0eNGhX114u3Fi1a+NieJo7Ymjhxoo/HjBkT8XFz5sxR7cxqZ7JTV5PVx77yyitZfk7ElqzjskdFSIlSc2MxggMAAIJDBwcAAAQnyCmq6tWrq/Y999wT8bF2+a+dskLu2N1NH3nkkai/Zvfu3XP0c1euXPFxZsPrs2fPVu21a9dGfOyyZctydC3JqkuXLj62U8Xr16/38dKlS2N2TalqxowZPh48eLDKlSlTJuqvf+TIER9v27ZN5fr06eNjO42MxCFPF7cnjScDRnAAAEBw6OAAAIDg0MEBAADBCaYGR55cvHDhwoiPs3PRdokk8lbXrl1Ve8iQIT6WxyZ8k7vuusvH2VnePWnSJNXes2dPxMdOnz7dx/IUX0RWpEgR1X7wwQcjPlaeXGyPtEDe27t3r4979Oihcp07d/bxgAEDovL6cluE8ePHR+U1EF2FChWKmEvUE8QlRnAAAEBw6OAAAIDgpGVn6VdaWlrCrhOTw6FDhw6N+LjGjRurdmZLfBNZRkZG5G0lM5HI9zDVhHAP7TTjkiVLfHz48GGVe+KJJ3wc0Mnq6zIyMhpl94cS6R62b99eteUSbnuyt9wiwZ40bne63bp1q4/T09NzfZ1RlPT3MFrkTuz58+uKlt/+9rc+fvHFF2N2TRFc9x4yggMAAIJDBwcAAASHDg4AAAhO0i4Tl6cWO+fcz372szhdCZC6Ll++rNrNmzeP05UgpxYsWJBpG6lrzZo1Ph47dqzKLVq0KNaXk22M4AAAgODQwQEAAMFJ2imqli1bqnaxYsUiPlaeEH727NmoXRMAAKGw2wQkG0ZwAABAcOjgAACA4NDBAQAAwUnaGpzMbNy4UbUfeOABHx8/fjzWlwMAAGKMERwAABAcOjgAACA4wZwmnmpCOIk61XEPg8BJ1MmPe5j8OE0cAACkBjo4AAAgOHRwAABAcLK7TPyoc25vNC4E2VI5Fz/LPUwM3MMw5PQ+cg8TB/cw+V33HmaryBgAACAZMEUFAACCQwcHAAAEhw4OAAAIDh0cAAAQHDo4AAAgOHRwAABAcOjgAACA4NDBAQAAwaGDAwAAgkMHBwAABIcODgAACA4dHAAAEBw6OAAAIDh0cAAAQHDo4AAAgODQwQEAAMGhgwMAAIJDBwcAAASHDg4AAAgOHRwAABAcOjgAACA4dHAAAEBw6OAAAIDg0MEBAADBoYMDAACCQwcHAAAEhw4OAAAIDh0cAAAQHDo4AAAgOPmz8+C0tLSMaF0IsicjIyMtJz/HPUwc3MMgHM3IyCiT3R/iHiYU7mHyu+49ZAQHAHJub7wvALnGPUx+172HdHAAAEBwsjVFBURT/vxfvx0LFCigcjfeeKOPL1++rHK2feXKFR9nZDCKDACpiBEcAAAQHDo4AAAgOExRIW4KFy6s2tWrV/dxpUqVVK5cuXI+3r9/v8rt2bNHtQ8cOODjCxcuqBxTVgCQGhjBAQAAwaGDAwAAgkMHBwAABIcaHETVDTfoPnT58uV9PGzYMJVr0qRJxJ+T9u7dm2l7w4YNPp4xY4bKnTlzxsfU4wDZI7dvKFmypMrJ7Rmcc+7kyZM+vnr1alSvC7geRnAAAEBw6OAAAIDgMEWFPJeW9vUZkrfeeqvKPfvssz5+/PHHIz6HXQq+b98+H9tl4XKXY+eca9eunY+PHz+ucgsXLvTxxYsXI74+4ktOUdr7K6dC7LQI8ta3vvUt1X7uued83K1bN5Wzn0s5Bb169WqVY3oYscAIDgAACA4dHAAAEBw6OAAAIDgJV4Mj6zcs5m2Tw5133unjCRMmqFyzZs18nC9fPpU7deqUjw8dOqRyy5cv9/Hp06dVrm7duqp91113+bhXr14qJ2sBqMFJHPZzL2u3ZE2Vc87Nnz/fx7ZWC7l38803+3jMmDEqJz9Pcsm4c86VLVtWtXv37u3jzZs3q9z58+d9zPd67hUsWFC1ixUr5mNbhxiN37f9/Mr2V199leevl1WM4AAAgODQwQEAAMGJ+xSVnaaQp0jnz68v7+zZsz6WQ5zO6ekGu3Q0nkNkqcAOVb/44os+btCgQcSfsyd9r1ix4rqxc86lp6f7WA6hO+dc8eLFI7btsLl9vyEx2Hs4fPhwH8spR+ecW7NmjY+ZosoZOYUgpzOcc27EiBE+tls52CX7meXq1Knj4/r166vc+vXrfWy/B5A1cgl/hw4dVO7w4cM+XrJkicrl1RSV/C4tXbq0ysm/43aneTllFu0drhnBAQAAwaGDAwAAgkMHBwAABCcuNThyG3Y5V+eccyNHjvRx4cKFVU7W3fz73/9WuV27dvl4+/btKnfp0iUf23ocmXNOL1W2ucy2hZf1QufOnVO5EGuA5By+rZ+Q93fr1q0qt23bNh9/8sknKrdu3ToflypVSuUqVqzoY7ss3B4HIe3cuVO15WniiB9bC9WlSxfVbtWqlY+PHj2qctzD7LPLeMuUKePjp556SuVkPYetr8usZuI///mPasvvzz59+qjcG2+84eMNGzao3LFjx3zMEvKv2Rqn8ePH+7hy5coqN3jwYB/n1e9Qfq87p2sh5es551yTJk18PG3aNJWbNGmSj6Ndf8UIDgAACA4dHAAAEJy4LxO3y8tuv/12H9tl4nKITD7OOT0MZ6eW5FCaHWK1w99yx9yNGzeqnFymftNNN6mc3L135syZKvf555/7+PLlyy4E8vd98uRJlRswYICPjxw5onJyCtBO+clh9HLlyqmcnIZq1KiRytnHyqHyjz76SOXYvTgx2J1XW7RoodpyauTgwYMqZ99v+GZ2SlBOIXTv3l3l7HeyJD+zdurdfp5Llizp46pVq6rcLbfc4uPFixernJzC+PLLLyNeS6qxf3OqVavmY/s3T26fkFdTVPZ55JRZrVq1VE6+h8qXL69ysdyqgxEcAAAQHDo4AAAgOHRwAABAcOJeg2Pn0+Vyb7tEUS5Httv1yy357fy+fKxdyli0aFHVlnk7rylfw84py5oguYW2c86NGjXKx6HU4Eh27n3Lli05eh5Zg2N/T/J3etttt6mcfZ/I99CcOXNULsTff7KQ99few8aNG0f8OXsSNcvEs69EiRKqLU8Ft/UT8vNkaxZPnz7tY1mT6Ny1NTmFChXysazHcU7XLNqfk9t8zJ49W+WivbV/opGfmbZt26qc/Bu0dOlSlZNHNUSL/Hto30Py3lux/A5mBAcAAASHDg4AAAhOXKao5JCkPWl0yJAhPrZTD3KY0+6ALE+ttjvryikqOcTq3LXTK3II1A6rymF0u3Ok3HXZ/hzTItlndyfu1KmTj+2OnnLnU+ecGzdunI/tMnXEj1weak8/tqe+y+0bFixYoHJ8nrJGbqthl+E3bdrUx/bzJL8D5c7jzjk3evRoH8tToZ1zrkqVKqrdpk0bHzdr1kzlZCmAXZYuf27t2rUqJ7cMyGxn+VDIHad/8YtfqJy8b++//77KReMzYncybt68uY9tWYacvlyxYkXUry0SRnAAAEBw6OAAAIDg0MEBAADBifsycTsfZ7dll9LT031sj1GYO3dull7PbhNt55/l0rsHHnhA5WRtT5EiRVRO1va8/vrrKkfNQNbIozkeeughlatTp46P7XELCxcuVO1FixZF4eqQW/Iz07JlS5WzS4XlSfOffvqpynHCdNbIpbqyhs0550qVKuVju/T6iy++8PHTTz+tcps2bfKxvWfymBvnnFu5cqWP69atq3IdO3b0sa1nlEcQ/OhHP1K5l19+2cch1tfZv0/ylG55vIVzzn344Yc+fvvtt6N7Ye7arQbat2/vY/uZ3LFjh49Xr16tcvZ9E02M4AAAgODQwQEAAMGJ+xRVTtkhsawOW9vhMTt9tHv3bh/Xrl1b5eQUlV0yJ3fvtdNnuD65S6dzzjVs2NDH9oRjOX21b98+lZs8ebJqnzt3Lo+uEHlJbvsgp0icu3aH8Y8//tjH3M+cqVChgo/r1asX8XF2m4Xnn3/ex3Kq0LnsTS/IaYrPP/9c5S5cuODjQYMGqZz83pU7Hjunv2fttEwIU5fFihVTbTmVa6cS5e7F58+fj+6FuWu3AahRo4aP7d9ROV1pt2aJJUZwAABAcOjgAACA4NDBAQAAwUnaGpxYsCcey+2o7YnGr776qo+pGcgaeyL8H//4Rx9XrFhR5eTScLlU1Dnn1qxZo9qxXIaIrJN1IHIpsHPOnThxQrWXLVvm41TYkj8v2LpAeRyD/TzJ+je5/YZzzs2ZM8fHufksyZqYS5cuqdyePXt8bE+elkfd2G085L9p+vTpEV8vmch7YZfTFy1a1Mf2WCN5jEa0vvNk3Vzfvn1Vrnz58j627yF5Cnw8P7+M4AAAgODQwQEAAMFhisqQp7c+8cQTKieX6U2bNk3lPvjgg+heWCDkqet//etfVU6eCG+XHcrdiadOnapyTEklJrsra69evXwsh76du/bU6J07d0bvwgJlp3rkTux253U5bSCnpJzTJ0HnFbslRKVKlXwsp6Sc09NSdtpNbhGRrFNSlvzdyO9H5/QS6w0bNqjc/v37r/sceUneJzk96Jx+D9ldrHft2hWV68kuRnAAAEBw6OAAAIDg0MEBAADBSfkaHDt32a1bNx/ffvvtKieXNr722msqZ7eax/8ULFhQtUeMGOFjeRqtc7qWZv369SrXr18/H0ejRgB5z54+3Lx5cx/bGqslS5aotj0xHtcnv7/kNhbO6a307Tb/x48f97Hc8j9abJ2NPIrFbhchvwfsNv/z58/3cSg1OJLdLuHgwYM+tp+J7373uz6WR5s4p4/GsM+ZWc2ireOSS8PLli2rcvK4jXXr1qmc/XzHCyM4AAAgOHRwAABAcOjgAACA4KR8DY7dwlzufWO3mJ43b56PZT2Oc2HOB+cF+/uVdTd2G3ZZW2O3YbfzyEh8tu5C1mPZrfvltvPOsbdRTtg9Y+TvUNZLWLVr11btTz75JEs/903k/e7Ro4fKdejQ4bqPc07Xmnz00UcqJ/fBCYX823HkyBGVk7VTjRo1UrmuXbv62NYlyhocWbfknK5vtH/jmjRpotpPPvmkj22tlHy/ffnllyqXKH8PGcEBAADBoYMDAACCk3JTVHaL+BdeeEG15Qmp9pTqUaNG+dgOseNrcuiyS5cuKieX3tsl+nLab9asWSqXSFMW9roTZTg2EcjfjT0xXG5Df+jQIZWzpxEj++x3m/yOOnfunMrJ92yxYsVUTi4Vzs4UlT2aQ372hw4dqnKlSpXysV1SLI8gGDdunMrlZsosUcl7ceDAAZVbsWKFj+XRG87pbQGKFy+ucuXKlfNx9erVVU5O99tSC7vVwE033eRje3/lZ/3w4cMqlyjfiYzgAACA4NDBAQAAwaGDAwAAgpMSNTj583/9zxwyZIjKtWrVSrXlFub2sXbbcFyfnF/v2bOnysl7YZcoTp061cdHjx5VuVjM6craIVvPIJe0FylSJOJz2BoBuXwzkeqIokXO07du3Vrl5HJgu6z05MmTUb2uVGCPY5A1OHabf1kzcebMGZWT9Rx2+bH8HNrl3VWrVlXtYcOG+bh06dIqJ+s3bH3QP//5Tx/LGhT7+iGy9UiTJ0/28aJFi1ROHndht2SoUKGCj+0xQseOHfOxXE7u3LW1PPfee6+P7f2Wn1n7fZ0oGMEBAADBoYMDAACCE+QUlZ1ekEOlP/3pT1XOThtMmTLFxxs3bozC1YXHLpu+7bbbfGyHPOW0lJ3OKVq0qI/tNJB8rB2Kt68vp8FkbK9HbgngnHN33HGHj+3JuZUqVfKxPXFXvv67776rcu+9956P7VB8iORQeYMGDVROfta2bNmicna6Elkjp2zs71C27WdE7kp73333qZz8/rSnVEt2Z9127dqptlyqbD+zcrrf7mL91ltv+ThRTqWOl1OnTvl406ZNKrd582Yf2/srp4oz2+Ha5uyWAfI70u5GLac2E/XzywgOAAAIDh0cAAAQHDo4AAAgOEHW4Mi5X+ecu//++31sl7p99tlnqv2vf/3Lx6EvSYwWWfdi595l29ZKPf744z62c8pyLt7m5PEPzuntxe1jb731Vh/Lmh/ndN2PnVOW7wV7TId8rF3uvGzZMh+HWINj5/DlstJatWqpnLzf9t4j9zJ7z9qaNlljJuvLnHOuTZs2PrbLy2X9mf38ZFYHYuvtNmzY4ONJkyap3M6dOx2+mby/9m9VTreksN9tf/nLX3zcuXNnlZP3374XEgUjOAAAIDh0cAAAQHCCmaKSw6GdOnVSuTp16vjYDqMuWLBAtXfv3h2Fq0stclh769atKienr+w0hVy62qtXr4g5e6ptZssg7TJT+bN2+kz+3Pnz51VO7uhqp6HkTqF211DbDo1dMt+/f38f291VJbucPlGXmSYTu+vw3r17fVy7dm2Vk1P1dtpefkbt96X9rEl2WkROS9nl5s8884yP7TLxVNjxO1HZqS55v+1nXZ48nqj3jBEcAAAQHDo4AAAgOHRwAABAcIKpwZFLw7///e+rnFzCduTIEZX729/+ptqpvjV4Tth5W7nMU861O+dc3bp1fVyxYkWVa968uY/tUn95jIKtC7BLUE+cOOFjuw2ArFOw9VbyRNwdO3aonKy7sacvy3//oUOHVM7W8oRA/v5LlCihcnJrd7vkdPny5T5+//33o3R1qcvWe02YMMHHtsapY8eOPrZHlmS2zb9k6y6OHz+u2rK+ceDAgSonT7RmO47EJU+It8feyO9deTp9ImEEBwAABIcODgAACE7STlEVLlxYtXv37u1jeZq1c3p4dsmSJSr3xRdfROHqUpscuty1a5fKZbYM/89//rOP7RJy2bZD2nb4XU4z2qXgme3+ieyzS5Nnz57tYztlMnr06Ig55J59P8tp1uHDh6uc3LG9e/fuKte6dWsfy6XAzulpqZUrV6rcm2++qdpLly71sTwVG4nLbsHRokULH9vP7IwZM3xsvwcSBSM4AAAgOHRwAABAcOjgAACA4CRVDY5cnlq/fn2V69u3r49Lly6tcpkt47VLWRFdmdW9yNoZlusnLnkPbW3F888/72O7jNjWQyF27HYF8pT7FStWqJzcVsMuE5f1dfYzau83NW7Jx9bgyM/3hx9+qHJjxozxMUc1AAAAxAgdHAAAEJyknaKqWbOmysldaO1wmVya/NJLL6kcpxgDeYepxeQgp4/sdyBLulOX/fz+4Q9/8LEt57A7yCciRnAAAEBw6OAAAIDg0MEBAADBSaoaHLlkMT09XeVee+01H9tTTxcvXuxju0ycpYwAAFz79zDZ67EYwQEAAMGhgwMAAIKTVFNUcvn3qlWrVE62Mzttmt1UAQAIHyM4AAAgOHRwAABAcOjgAACA4GS3Bueoc25vNC4kK2QNzsWLF+N1GYmgci5+Nq73EB73MAw5vY/cw8TBPUx+172HaewDAwAAQsMUFQAACA4dHAAAEBw6OAAAIDh0cAAAQHDo4AAAgODQwQEAAMGhgwMAAIJDBwcAAASHDg4AAAjOfwGBB8eXj/ZLtQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test your new predictions\n",
    "Conv_predictions = Conv_AE.predict(X_test)\n",
    "draw_predictions_set(Conv_predictions, 'NB3-ConvAE-predictions')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
